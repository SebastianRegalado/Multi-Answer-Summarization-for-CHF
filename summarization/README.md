## Summarization

To summarize answers to questions based on their perspective, simply change the
file to classfiy in the `summarization_classified.py` file and run the script.

## Evaluation with SUPERT

We use a fork of the repository [SUPERT](https://github.com/yg211/acl20-ref-free-eval) to evaluate the quality of the summaries generated by our model. Note that the original repository is not compatible with Python 3.6+. A separate virtual environment is recommended to run the evaluation. Install the requirements as indicated in the evaluation folder. 

To run the evaluation, simply run the following command:

```
python evaluation/evaluate_summary.py [path-to-file-with-summaries] 
```